\chapter{Background}
\index{Backgroud}
This section will review the primary concepts that will be evaluated empirically through the experiments in Chapters \ref{chapter:objective_exploration}, \ref{chapter:loss_complexity}, and \ref{chapter:param_eff}

\section{Task Description}
\label{section:task_defs}
All experiments in this work are designed to estimate predictive performance on retrieval tasks.
This section formalizes the retrieval task and describes it in detail for the sake of clarity.
We also define terms that well be used for the rest of the paper.
Readers familiar with retrieval tasks are encouraged only to note the terminology we define in this section.

% First we will formally define terms, though, in the fully generalized setting of our task.
We are given a \underline{${\D}$}ataset, $\D$, containing a set of $K$ distinct information sources. 
We will refer to each information ;ource as a \textit{\underline{$\mathcal{V}$}iew}.
Section \ref{section:multiview_framework} discusses this convention further.

% These views are distinguishable, of course, but not necessarily ordered.
We will refer to an arbitrary view as $\V[j]$ where $j\in[1,K]$ indexes each view.
Each view, $\V[j]$,  contains $N$ samples and each sample, $\nu_{ij} \in \V[j], i\in [1,N], j\in[1,K]$ has a corresponding entry in each of the other $K-1$ views which share some sort of semantic relatedness.
Furthermore, each $V_j$ has it's own representation scheme for it's constituent samples making a conventional tensor representation with fixed dimensions for all samples in $D$ inexact.
This gives the generalized dataset for our task the following definition:
\begin{equation*}
    \D = \{(\nu_{i1}, \nu_{i2}, \cdots, \nu_{iK}) : \nu_{i1} \in \V[1], \nu_{i2} \in \V[2], \cdots, \nu_{iK} \in \V[K], \quad i \in [1,N]\}
\end{equation*}
To reduce ambiguity, we will refer to an arbitrary K-tuple in $\D$ as a \textit{datapoint} and an arbitrary member of a K-tuple/datapoint, $\nu_{ij}$, as a \textit{viewpoint}. 
We will be sure to clarify in cases where this definition of viewpoint allows for ambiguity between the colloquial use of viewpoint by instead using `point of view' for the colloquial sense as needed.

To be more concrete for a moment, in the dataset used for this work we have $K=4$ views in our dataset which are: 1) the set of images, 2) the English descriptions, 3) the Hindi descriptions, and 4) the Japanese descriptions (although we do not assume that particular ordering).
For each image (viewpoint) in the image view, there is a corresponding spoken utterance (viewpoint) describing that image in all three languages.
We therefore assume some type of reliable mutual semantic information between each viewpoint in all datapoints.

Also note that, on occasion, we further categorize our views based on \textit{modality}.
To be clear, this dataset contains two modalities: vision and audio; one view is in the vision modality (images) and three views are in the audio modality (English, Japanese, and Hindi utterances).
We therefore commonly differentiate retrieval results based on modality pairing, retrieval results from vision-audio pairs being referred to as `image retrieval' and audio-audio retrieval results referred to as cross-lingual.
This partitioning is made to enable readers to more clearly assess the the models' ability to capture the mutual information shared by each view.


As an aside, \textit{mutual information} (MI) has a well-known and well-defined mathematical form, but we will be using the term in the idiomatic sense.
Recent work by Tschannen et al. \cite{tschannen2019mutual} has shown that  many popular lower-bound maximization schemes for estimating MI break down in practice and do not correlate with expected performance gains.
Furthermore, MI is only well-studied in the two variable (view) case. 
The generalization of MI to three or more variables is referred to as \textit{interaction information} and has certain properties (such as permitting negative values) that make it difficult to interpret.


Keeping these points in mind, we still wish to encode the datapoints from each view into a shared vector space. 
Our task, then, in words is: given an input (indexed by i), $\nu_{ij}$, from the $j\th$ view, $V_j$, and a set of viewpoints from a target view $V_t$, we would like like to retrieve $\nu_{it}$ from $V_t$.
In order to do this we use an \textit{encoder} functions $f_{\theta_i}$ and $f_{\theta_t}$ to produce a numeric representation of the viewpoints from the source and target views respectively.
In this work all encoders take the form of neural networks, and in particular convolutional neural networks (CNNs).

The retrieval is then performed by comparing the input, $\nu_{ij}$, with all other $\nu_{ik} \in V_k$ and returning the $\nu_{ik}$ with the highest \textit{similarity score}.
This similarity score is measured by a chosen \textit{similarity function}, $S$.
$S$ follows the typical mathematical definition of a distance measure (i.e. it takes two arguments, it's always non-negative, and it is symmetric in it's arguments).
For retrieval tasks this similarity function typically takes one of the following forms: the dot-product, the euclidean distance, or the cosine similarity.

With all terms defined, we can write our most general leaning objective as:
\begin{equation}
    \underset{\theta}{\max} \Big(\sum_{(\nu_i1,\cdots, \nu_ik)\in D, i\in[1,N]} \;\; \sum_{j\in[1,K]} \underset{t\in[1,K], t\neq j }{\max} S(f_{\theta_j}(\nu_{i,j}), f_{\theta_t}(\nu_{i,t}, \,t))\Big)
\end{equation}

This optimization objective has no known exact solution and must be approximated.
Most commonly, this type of optimization problem is approximated using a differentiable loss function and a gradient-descent based optimization algorithm.
There are a number of loss-function in common use for this type of maximization objective, many of which are motivated by maximizing the estimate of the aforementioned MI estimate.
Which of these loss functions produces optimal results for our particular setting is the subject of the experiments in Chapter \ref{chapter:objective_exploration}.

\section{Loss Functions}
\label{chapter:background|section:loss_functions}
There are numerous loss functions that are designed to encourage `similarity' among pairs or groups of datapoints.
We explore a subset of these in this work an describe the important aspects of each below.
\subsection{Triplet}
\label{section:triplet}
Triplet loss has perhaps the longest history of use in this task.
Harwath et al. \cite{harwath2015deep} first introduced the task of spoken image retrieval and used a maximum margin objective between each final unpooled image embedding and final sequence embedding of paired images and utterances.
This obective eventually became formulated a the the triplet loss.

Generally, triplet losses take an \textit{anchor} embedding/vector $A$, \textit{truthy} embedding $T$ (which is positively associated with the anchor), and a \textit{falsey} embedding $F$, also known as an imposter, that is not associated with the anchor. 
Then, given a chosen similarity function, the triplet loss can be calculated as:
\begin{equation}
\label{orig_triplet_loss}
    Triplet(A, T, F) = \max(0, S(A, F) - S(A, T) +M)
\end{equation}
Where $M$ is some margin to be chosen a hyperparameter (often just 1). 
This loss effectively pushes the $A$ and $T$ embeddings toward each other and the $A$ and $F$ embeddings away from each other.
The margin, then, influences how stronly model is encouraged to push or pull the associted embeddings.

This was formalized in \cite{harwath2017unsupervised} for spoken image retrieval by using two triplet losses: one defined with image representaions as anchors, and one with spoken captions as anchors.
It can be written as:
\begin{align}
    \label{retrieval_triplet}
    T(I_j, C_j, I_j^{imp}, C_j^{imp}) &= \sum_{j=1}^N \Big(\max(0,S(I_j, C_j^{imp}) - S(I_j, C_j) +1) \\
                                      &{\quad\quad\quad+} \max(0,S(I_j^{imp}, C_j) - S(I_j, C_j) +1)\Big)
\end{align}
Where $S$ is a chosen similarity function,  $I_J$ and $C_j$ are the $j\th$  image/caption which are treated as , and $I_j^{imp}$ and $C_j^{imp}$ are the $j\th$ \textit{imposter} samples of an image and caption not associted with the correct image/caption pair.
In this formulation, the first $\max$ term is effectively the triplet loss when  $I_j$ is viewed as the anchor, $C_j$ is viewed as $T$, and  $C_j^{imp}$ is viewed as $F$.
The second $max$ term then uses $C_j$ as the anchor and the images in likewise fashion.
The imposters are sampled from a uniform distribution from within the minibatch.

This form of triplet loss is highly dependent on the imposter samples chosen, since in the final stages of optimization most negative samples will alredy be far from the anchor.
This motivates hard and semi-hard negative sampling \cite{jansen2018unsupervised} which, generally speaking, seek to sample imposter/negative samples that are very close to the anchor (and there for `hard' to distiguish from the positives).
This general approach was adapted to the spoken image retriaval task by Harwath et al. \cite{harwath2018jointly} which showed that the use of semi-hard negatives increased retriaval performance.

The accompanying loss function, was formulated more generally in \cite{harwath2019learning} as:
\begin{align}
    \label{eq:triplet_full}
    \mathcal{L}(\theta) &= T(I_j, C_j, I_j^{imp}, C_j^{imp}) + T((I_j, C_j, \tilde{I}_j^{imp}, \tilde{C}_j^{imp})
\end{align}
Where $T$ is the same as in Equation \ref{retrieval_triplet}. 
The important difference between the terms is that $\tilde{I}_j^{imp}$ and $\tilde{C}_j^{imp}$ are chosen to be the image and caption that are most similar to their respective anchors in the batch (as measured by the similarity function $S$),
instead of the uniform distribution as $I_j^{imp}$ and $C_j^{imp}$


\subsection{Masked Margin Softmax (MMS)}
\label{section:mms_loss}
The Masked Margin Softmax (MMS) loss was introduced by Ilharco et al. \cite{ilharco2019large} and uses many more negative examples than triplet loss.
MSS loss effectively computes the categorical cross-entropy loss between an anchor embedding and $K$ other embeddings, only one of which is the positive corresponding embedding.
Letting $B$ be the batch size and $X$ and $Y$ be the final representations of $B$ samples of two arbitrary informations sources.
We use information souce to refer to one of the followinge: an image, or a speech utterance from a language in the set of $\mathcal{D}_{lang}$ languages in the dataset. 
MMS can then be written for a single minibatch as:
\begin{align}
    \label{MMS_base}
    MMS(X,Y) = \frac{1}{B}\sum_{j=1}^B\frac{e^{S(X_j, Y_j)-M}}{e^{S(X_j, Y_j)-M} +\sum_{k=1}^{B} \mathbb{I}[k!=j \land k \notin \mathcal{Z}]e^{S(X_j, Y_k)}}
\end{align}
Where $\mathcal{Z}$ is a set of predefined indices corresponding to examples to be \textit{masked} (hence the name).
In our dataset there are no examples of this sort so $\mathcal{Z}$ is always empty.

Notice that $MMS(X, Y)$ is not symmetric. 
Specifically, each representation $X_j$ is contrasted with all other $Y_k$ in the batch, by $Y_j$ is not given the same treatment.
The full MMS loss for a batch completes the symmetry and is defined as:
\begin{align}
    \label{eq:mms}
    \mathcal{L}_{MMS}(\theta) = MMS(X,Y) + MMS(Y,X)
\end{align}

The margin $M$ is an important quantity and can be a constant value, updated according to a schedule, or adaptively updated according to some function of the batch.
When $M=0$ this loss has the form of what is often referred to in the literature as InfoNCE loss \cite{oord2018representation}.
Several works use this terminology \cite{he2020momentum, hjelm2018learning}, but there seems to be a lack of consensus over this issue \cite{tian2020contrastive}.
Still, we hesitantly follow the current conventions and whenever an experiment uses a margin of 0, we will refer to it as InfoNCE.

These methods are explored empirically in Section \ref{section:obj_exp_testing_rep}.

\subsection{Hypersphere Loss}
The hypersphere loss was introduced by Wang \cite{wang2020understanding} and attempts to reframe conventional contrastive loss through the notions of \textit{alignment} and \textit{uniformity} of latent representaions on the hypersphere.
They provide empirical evidence that directy optimizing their measures of alignment and uniformity can outperform contrastive losses.
Given a batch of size $B$ with $X$ and $Y$ defined in the same manner as in Section \ref{section:mms_loss}, the \textit{alignment} sub-loss is defined as:
\begin{align*}
    HSphere_{align}(X,Y) = \frac{1}{B}\sum_{i=1}^B \left\Vert X_i - Y_i\right\Vert_2^\alpha
\end{align*}
where $\alpha$ is a hyperparameter to be tuned.

The \textit{uniformity} sub-loss is calculated as:
\begin{align*}
    HSphere_{uniformity}(X,Y) = \frac{1}{B(B+1)/2}\sum_{i=1}^B\sum_{j\geq i}^B e^{-t\left\Vert X_i - Y_j\right\Vert_2^2}
\end{align*}
with $t$ being another hyperparameter.

In the original work the showed that values of $\alpha$ and $t$ around 1 tended to work well, but $t$ often needed to be lower than $alpha$.

With those defined the full hyperspheric loss is defined as:

\begin{align}
    \label{eq:hyperspheric}
    HSphere_{full}(X,Y) = \lambda_a*HSphere_{align}(X,Y) + \lambda_u*HSphere_{uniformity}(X,Y)
\end{align}
With $\lambda_a$ and $\lambda_u$ being additional weiting parameters.
In the original work, an additional baseline contrastive loss was also added to Equation \ref{eq:hyperspheric} with an accompanying weighting hyperparameter, but they found that it was not necessary for optimal performance so we have omitted it in our experiments for simplicity.


\section{Multiview Contrasting Frameworks}
\label{section:multiview_framework}
Work by Tian et al. \cite{tian2020contrastive}, provided a framework for using contrastive loss functions with several \textit{views}.
They define a view as a certain sensory input during an arbitrary event.  
In this way, there may be multiple views all describing the same underlying (or latent) event.
A view, then, can have a more abstract meaning in which some amount of information contained in the view is shared among other views.

Their work, then attempts to devise appropriate training schemes to maximize the latent information shared by all views.
The two components of their work relevant to ours are the \textit{full-graph} and \textit{anchor} training schemes.

Both schemes start with a symmetric loss function, $\mathcal{L_{sym}(V_1,V_2}$, defined over two views, $V_1$ and $V_2$, of size $B$.
Each samples in $V_1$ are assumed to have a corresponding positive sample in $V_2$ that carries the same mutual information (and vice-versa for the samples in $V_2$).
Note that $V_1$ and $V_2$ are from a minibatch of the full dataset, but to reduce notational clutter we will only define a single batch update.
$\mathcal{L_{sym}}$ is also assumed to be designed such that it encourages alignment between the positive examples in two views and some notion of distance between the examples that do not carry mutual information.
We will assume as well that the positive examples share the same index in the batch. 
Notice that Equations \ref{eq:hyperspheric}, \ref{eq:triplet_full}, and \ref{eq:mms} all meet the above assumptions.

Finally, the full-graph and anchor frameworks attempt to maximize the mutual information contained in a set of $K$ views $\mathcal{V}$.

% \subsection{Full Graph}
% \label{section:full_graph_framework}
With the above assumptions and notation, the full graph framework computes the following loss for each batch:
\begin{align*}
    \mathcal{L}_{full_g}(\theta) = \frac{1}{B(B+1)/2}\sum_{i=1}^B \sum_{j\geq k}^B \mathcal{L}_sym(V_i, V_j)
\end{align*}

The anchor framework can be defined intuitively as aligning all views to one `anchor' view in a hub-and-spoke manner.
It is calculated as:
\begin{align*}
    \mathcal{L}_{anchor}(\theta, i) = \frac{1}{B-1}\sum_{j=1}^{B} \mathbb{I}[j\neq i] \mathcal{L}_sym(V_i, V_j)
\end{align*}
Where $i$ denotes the index of the anchor view.
% \subsection{Anchor}
% \label{section:anchor_framework}
\section{Neural Architectures}
\label{background:model_architectures}
We use convolutional neural networks (CNN) as our base architecture. 
The focus of this work is on the effect of loss functions and training regime in a multilingual image retrieval and as such does not attempt to optimize architecture.
We therefore use two off the shelf encoders for our images and speech utterances.
\subsection{Image Model}
The image encoder is based on the ResNet50 model proposed by He et al. \cite{he2016deep}.
It is pretrained on ImageNet classification, but the final layer has been altered slightly.
In the same manner as Harwath et al. \cite{harwath2019learning}, we remove the final softmax and fully connected layer and instead perform a 1x1 convolution to obtain a desired size of the final imagage representations.
The output representation size for each image is a 7x7x1024 tensor.
\subsection{Audio Model}
\label{chapter:background|section:audio_model}
We will first describe the pre-processing steps taken on the audio and then briefly describe the architecture.
\subsubsection{Audio Pre-Processing}
The same pre-processing is used for all experiments.
Log-Mel filter bank spectrogram features are calculated from raw audio with 40 frequency bins.
We use Hamming-windowed frames with a 15 ms width and shift of 10 ms.
During batching, all utterances are padded up to the longest utterance in the batch.
\subsubsection{Architecture}
The audio/speech model we use the same as \cite{harwath2019learning}. 
It is a 17 layer CNN with the first being a 1D convolution of 128 filters of size 1x40x1 across the time dimension.
The following 16 layers divided into 4 residual \textit{speech} blocks.
Each speech block starts with an initial 1D convolution layer with kernel size of 1x9 a stride of two that effectively downsamples the input followed by a batch norm layer.
This followed by three successive 1D convolution (kernel size 1x9, stride 1) and batch norm layers.
No padding is used.
This produces 1024 dimensional embeddings of various length depending on the batch.



% \section{Multi-lingual Training Frameworks}
% \label{section:multiling_framework}
%
% \subsection{Language Tag}
% \subsection{Shared Layers}

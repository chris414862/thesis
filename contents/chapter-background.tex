\chapter{Background}
\index{Backgroud}
This section will review the primary \dftwrds that will be compared empirically through the experiments in \dftwrds{put in refs}

\section{Task Description}
\label{section:task_defs}
All experiments in this work are designed to estimate predictive performance on retrieval tasks.
This section formalizes the retrieval task and describes it in detail for the sake of clarity.
We also define terms that well be used for the rest of the paper.

% First we will formally define terms, though, in the fully generalized setting of our task.
We are given a \underline{${\D}$}ataset, $\D$, containing a set of $K$ distinct \textit{\underline{$\mathcal{V}$}iews}.
% These views are distinguishable, of course, but not necessarily ordered.
We will refer to an arbitrary view as $\V[j]$ where $j\in[1,K]$ indexes each view.
Each view, $\V[j]$,  contains $N$ samples and each sample, $\nu_{ij} \in \V[j], i\in [1,N], j\in[1,K]$ has a corresponding entry in each of the other $K-1$ views which share some sort of semantic relatedness.
Furthermore, each $V_j$ has it's own representation scheme for it's constituent samples which making a conventional tensor representation with fixed dimensions for all samples in $D$ inexact.
This gives the generalized dataset for our task the following definition:
\begin{equation*}
    \D = \{(\nu_{i1}, \nu_{i2}, \cdots, \nu_{iK}) : \nu_{i1} \in \V[1], \nu_{i2} \in \V[2], \cdots, \nu_{iK} \in \V[K], \quad i \in [1,N]\}
\end{equation*}
To reduce ambiguity, we will refer to an arbitrary K-tuple in $\D$ as a \textit{datapoint} and an arbitrary member of a K-tuple/datapoint, $\nu_{ij}$, as a \textit{viewpoint}. 
We will be sure to clarify in cases where this definition of viewpoint allows for ambiguity between the colloquial use of viewpoint by instead using `point of view' for the colloquial sense as needed.

To be more concrete for a moment, in the dataset used for this work we have $K=4$ views in our dataset which are: 1) the set of images, 2) the English descriptions, 3) the Hindi descriptions, and 4) the Japanese descriptions (although we do not assume that particular ordering).
For each image (viewpoint) in the image view, there is a corresponding spoken utterance (viewpoint) describing that image in all three languages.
We therefore assume some type of reliable mutual semantic information between each viewpoint in all datapoints.

As an aside, \textit{mutual information} (MI) has a well-known and well-defined mathematical form, but we will be using the term in the idiomatic sense.
Recent work by \citeme{} has shown that  many popular lower-bound maximization schemes for estimating MI break down in practice and do not correlate with expected performance gains.
Furthermore, MI is only well-studied in the two variable (view) case. 
The generalization of MI to three or more variables is referred to as \textit{interaction information} (II) and has certain properties (such as permitting negative values of II) that make it difficult to interpret.

Also note that, on occasion, we further categorize our views based on \textit{modality}.
To be clear, this dataset contains two modalities: vision and audio; one view is in the vision modality (images) and three views are in the audio modality (English, Japanese, and Hindi utterances).
We therefore commonly differentiate retrieval results based on modality pairing, retrieval results from vision-audio pairs being referred to as `image retrieval' and audio-audio retrieval results referred to as cross-lingual.

Keeping these points in mind, we still wish to encode the datapoints from each view into a shared vector space. 
Our task, then, in words is: given an input (indexed by i), $\nu_{ij}$, from the $j\th$ view, $V_j$, and a set of datapoints from a target view $V_t$, we would like like to retrieve $\nu_{it}$ from $V_t$.
In order to do this we use an \textit{encoder} function $f_\theta$ to produce a numeric representation of a viewpoint.

$f_\theta$ is parameterized by an arbitrary number of parameters denoted by $\theta$.
It takes a viewpoint as input, and produces an output representation.
In this work all encoders take the form of neural networks, and in particular CNNs \dftwrds[reference descrip in background].
We also allow the encoder to take as input information indicating the input view.
That is, if we let $j$ denote the input view, we can denote the encoder and it's input/arguments as $f_\theta(\nu_{ij},j)$.
This notation effectively generalizes over many potential implementations of $f_\theta$. 
$f_theta$ might be implemented such that all views have a distinct set of model parameters, or some parameters might be shared across a subset views.


The retrieval is then performed by comparing the input, $\nu_{ij}$, with all other $\nu_{ik} \in V_k$ and returning the $\nu_{ik}$ with the highest \textit{similarity score}.
This similarity score is measured by a chosen \textit{similarity function}, $S$.
$S$ follows the typical mathematical definition of a distance measure (i.e. it takes two arguments, it's always non-negative, and it is symmetric in it's arguments).
For retrieval tasks this similarity function typically takes one of the following forms: the dot-product, the euclidean distance, or the cosine similarity.

With all terms defined, we can write our most general leaning objective as:
\begin{equation}
    \underset{\theta}{\max} \Big(\sum_{(\nu_i1,\cdots, \nu_ik)\in D, i\in[1,N]} \;\; \sum_{j\in[1,K]} \underset{t\in[1,K], t\neq j }{\max} S(f_\theta(\nu_{i,j}, \,j), f_\theta(\nu_{i,t}, \,t))\Big)
\end{equation}
Note, again, that $f_\theta$ need not be a single neural network encoder.

This optimization objective has no known exact solution \dftwrds[find support or use other language to describe difficulty] and must be approximated.
Recently, this type of optimization has been routinely  approximated using a differentiable loss function and a gradient-descent based optimization algorithm.
There are a number of loss-function in common use for this type of maximization objective, many of which are motivated by maximizing the estimate of the aforementioned MI estimate.
Which one of these loss functions produces optimal results for our particular setting is the subject of the following experiments in this chapter.

\section{Loss Functions}
There are numerous loss functions that are designed to encourage `similarity' among pairs or groups of datapoints.
We explore a subset of these in this work an describe the important aspects of each below.
\subsection{Triplet}
Triplet loss has perhaps the longest history of use in this task.
Harwath et al. \cite{harwath2015deep} first introduced the task of spoken image retrieval and used a maximum margin objective between each final unpooled image embedding and final sequence embedding of paired images and utterances.
This obective eventually became formulated a the the triplet loss.

Generally, triplet losses take an \textit{anchor} embedding/vector $A$, \textit{truthy} embedding $T$ (which is positively associated with the anchor), and a \textit{falsey} embedding $F$, also known as an imposter, that is not associated with the anchor. 
Then, given a chosen similarity function, the triplet loss can be calculated as:
\begin{equation}
\label{orig_triplet_loss}
    Triplet(A, T, F) = \max(0, S(A, F) - S(A, T) +M)
\end{equation}
Where $M$ is some margin to be chosen a hyperparameter (often just 1). 
This loss effectively pushes the $A$ and $T$ embeddings toward each other and the $A$ and $F$ embeddings away from each other.
The margin, then, influences how stronly model is encouraged to push or pull the associted embeddings.

This was formalized in \cite{harwath2017unsupervised} for spoken image retrieval by using two triplet losses: one defined with image representaions as anchors, and one with spoken captions as anchors.
It can be written as:
\begin{align}
    \label{retrieval_triplet}
    T(I_j, C_j, I_j^{imp}, C_j^{imp}) &= \sum_{j=1}^B \Big(\max(0,S(I_j, C_j^{imp}) - S(I_j, C_j) +1) \\
                                      &{\quad\quad\quad+} \max(0,S(I_j^{imp}, C_j) - S(I_j, C_j) +1)\Big)
\end{align}
Where $S$ is a chosen similarity function,  $I_J$ and $C_j$ are the $j\th$  image/caption which are treated as , and $I_j^{imp}$ and $C_j^{imp}$ are the $j\th$ \textit{imposter} samples of an image and caption not associted with the correct image/caption pair.
In this formulation, the first $\max$ term is effectively the triplet loss when  $I_j$ is viewed as the anchor, $C_j$ is viewed as $T$, and  $C_j^{imp}$ is viewed as $F$.
The second $max$ term then uses $C_j$ as the anchor and the images in likewise fashion.
The imposters are sampled from a uniform distribution from within the minibatch.

This form of triplet loss is highly dependent on the imposter samples chosen, since in the final stages of optimization most negative samples will alredy be far from the anchor.
This motivates hard and semi-hard negative sampling \cite{jansen2018unsupervised} which, generally speaking, seek to sample imposter/negative samples that are very close to the anchor (and there for `hard' to distiguish from the positives).
This general approach was adapted to the spoken image retriaval task by Harwath et al. \cite{harwath2018jointly} which showed that the use of semi-hard negatives increased retriaval performance.

The accompanying loss function, was formulated more generally in \cite{harwath2019learning} as:
\begin{align*}
    \mathcal{L}(\theta) &= T(I_j, C_j, I_j^{imp}, C_j^{imp}) + T((I_j, C_j, \tilde{I}_j^{imp}, \tilde{C}_j^{imp})
\end{align*}
Where $T$ is the same as in Equation \ref{retrieval_triplet}. 
The important difference between the terms is that $\tilde{I}_j^{imp}$ and $\tilde{C}_j^{imp}$ are chosen to be the image and caption that are most similar to their respective anchors in the batch (as measured by the similarity function $S$),
instead of the uniform distribution as $I_j^{imp}$ and $C_j^{imp}$


\subsection{InfoNCE}
The InfoNCE loss was introduced by Oord et al. \cite{oord2018representation} and when applied to retrieval tasks uses many more negative examples than triplet loss.
InfoNCE loss effectively computes the categorical cross-entropy loss between an anchor embedding and $K$ other embeddings, only one of which is the positive corresponding embedding.
It can be written as:
\begin{align*}
\mathcal{L}(\theta) = \mathcal{L}_{contrast}(V_1, V_2)+ \mathcal{L}_{contrast}(V_1, V_2)
\end{align*}

    % \underset{}{\mathbb{E}}\[\frac{e^{S(V_1,C_j}}{}\]
We are careful to note here that V

\subsection{Masked Margin}
\subsection{Scheduled Masked Margin}
\section{Multiview Contrasting Frameworks}
\label{section:backg_contrast_framework}
\subsection{Full Graph}
\label{section:full_graph_framework}
\subsection{Anchor}
\label{section:anchor_framework}
\section{Neural Architectures}
\subsection{ResDaveNet}
\subsection{Pooling}
\subsubsection{Multiheaded Attention Pooling}
\subsubsection{Average Pooling}
\section{Multi-lingual Training Frameworks}
\subsection{Language Tag}
\subsection{Shared Layers}
